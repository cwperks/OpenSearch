/*
 * SPDX-License-Identifier: Apache-2.0
 *
 * The OpenSearch Contributors require contributions made to
 * this file be licensed under the Apache-2.0 license or a
 * compatible open source license.
 *
 * Modifications Copyright OpenSearch Contributors. See
 * GitHub history for details.
 */

/*
 * Licensed to Elasticsearch under one or more contributor
 * license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright
 * ownership. Elasticsearch licenses this file to you under
 * the Apache License, Version 2.0 (the "License"); you may
 * not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing,
 * software distributed under the License is distributed on an
 * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
 * KIND, either express or implied.  See the License for the
 * specific language governing permissions and limitations
 * under the License.
 */

import org.apache.tools.ant.taskdefs.condition.Os
import org.opensearch.gradle.info.BuildParams
import org.opensearch.gradle.test.RestIntegTestTask
import org.opensearch.gradle.MavenFilteringHack

import java.nio.file.Files
import java.nio.file.Path
import java.nio.file.Paths

import static org.opensearch.gradle.PropertyNormalization.IGNORE_VALUE

apply plugin: 'opensearch.rest-resources'
apply plugin: 'opensearch.rest-test'

opensearchplugin {
  description = 'The HDFS repository plugin adds support for Hadoop Distributed File-System (HDFS) repositories.'
  classname = 'org.opensearch.repositories.hdfs.HdfsPlugin'
}

configurations {
  hdfsFixture
}

dependencies {
  api "org.apache.hadoop:hadoop-client-api:${versions.hadoop3}"
  runtimeOnly "org.apache.hadoop:hadoop-client-runtime:${versions.hadoop3}"
  api("org.apache.hadoop:hadoop-hdfs:${versions.hadoop3}") {
    exclude module: 'jetty-server'
    exclude group: 'org.codehaus.jackson'
  }
  api 'org.apache.htrace:htrace-core4:4.2.0-incubating'
  api "org.apache.logging.log4j:log4j-core:${versions.log4j}"
  api 'org.apache.avro:avro:1.12.1'
  api "com.google.code.gson:gson:${versions.gson}"
  runtimeOnly "com.google.guava:guava:${versions.guava}"
  api "commons-logging:commons-logging:${versions.commonslogging}"
  api 'commons-cli:commons-cli:1.11.0'
  api "commons-codec:commons-codec:${versions.commonscodec}"
  api 'commons-collections:commons-collections:3.2.2'
  api "org.apache.commons:commons-compress:${versions.commonscompress}"
  api 'org.apache.commons:commons-configuration2:2.13.0'
  api "commons-io:commons-io:${versions.commonsio}"
  api "org.apache.commons:commons-lang3:${versions.commonslang}"
  implementation 'com.google.re2j:re2j:1.8'
  api 'javax.servlet:servlet-api:2.5'
  api "org.slf4j:slf4j-api:${versions.slf4j}"
  api "org.apache.logging.log4j:log4j-slf4j2-impl:${versions.log4j}"
  api "net.minidev:json-smart:${versions.json_smart}"
  api "io.netty:netty-all:${versions.netty}"
  implementation "com.fasterxml.woodstox:woodstox-core:${versions.woodstox}"
  implementation 'org.codehaus.woodstox:stax2-api:4.2.2'

  hdfsFixture project(':test:fixtures:hdfs-fixture')
}

restResources {
  restApi {
    includeCore '_common', 'cluster', 'nodes', 'indices', 'index', 'snapshot'
  }
}

// Testcontainers-based fixture is always supported (requires Docker)
boolean fixtureSupported = true
boolean legalPath = rootProject.rootDir.toString().contains(" ") == false
if (legalPath == false) {
  fixtureSupported = false
  logger.warn("hdfsFixture unsupported since there are spaces in the path: '" + rootProject.rootDir.toString() + "'")
}

processTestResources {
  Map<String, Object> expansions = ['hdfs_port': '9999', 'hdfs_host': 'localhost']
  inputs.properties(expansions)
  MavenFilteringHack.filter(it, expansions)
}

normalization {
  runtimeClasspath {
    // ignore generated keytab files for the purposes of build avoidance
    ignore '*.keytab'
    // ignore fixture ports file which is on the classpath primarily to pacify the security manager
    ignore 'ports'
  }
}

tasks.named("dependencyLicenses").configure {
  mapping from: /hadoop-.*/, to: 'hadoop'
}

tasks.named("integTest").configure {
  it.dependsOn(project.tasks.named("bundlePlugin"))
}

testClusters.integTest {
  plugin(project.tasks.bundlePlugin.archiveFile)
}


// Create simplified HDFS fixture using Testcontainers
tasks.register('hdfsFixture', org.opensearch.gradle.test.AntFixture) {
  dependsOn configurations.hdfsFixture
  executable = "${BuildParams.runtimeJavaHome}/bin/java"
  env 'CLASSPATH', "${-> configurations.hdfsFixture.asPath}"
  maxWaitInSeconds = 60
  onlyIf { BuildParams.isInFipsJvm() == false }
  waitCondition = { fixture, ant ->
    return fixture.portsFile.exists()
  }
  args 'hdfs.MiniHDFS', baseDir
}

// Simplified integration test setup
integTest {
  onlyIf { BuildParams.isInFipsJvm() == false }
  dependsOn hdfsFixture
  systemProperty 'tests.rest.suite', 'hdfs_repository'
}

if (fixtureSupported) {
  // Standard HDFS integration test
  integTest {
    systemProperty 'tests.rest.suite', 'hdfs_repository'
  }
} else {
  // Basic plugin load test when fixture is not supported
  integTest {
    systemProperty 'tests.rest.suite', 'hdfs_repository/10_basic'
  }
}



thirdPartyAudit {
  ignoreMissingClasses()
  ignoreViolations(
    // internal java api: sun.misc.Unsafe
    'com.google.common.primitives.UnsignedBytes$LexicographicalComparatorHolder$UnsafeComparator',
    'com.google.common.primitives.UnsignedBytes$LexicographicalComparatorHolder$UnsafeComparator$1',
    'org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer',
    'org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer$1',
    'org.apache.hadoop.io.nativeio.NativeIO',
    'org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm',
    'org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm$Slot',

    // internal java api: sun.misc.SignalHandler
    'org.apache.hadoop.util.SignalLogger$Handler',

    'org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$UnsafeAtomicHelper',
    'org.apache.hadoop.hdfs.server.datanode.checker.AbstractFuture$UnsafeAtomicHelper$1',
    'org.apache.hadoop.service.launcher.InterruptEscalator',
    'org.apache.hadoop.service.launcher.IrqHandler',

    'com.google.common.cache.Striped64',
    'com.google.common.cache.Striped64$1',
    'com.google.common.cache.Striped64$Cell',
    'com.google.common.hash.LittleEndianByteArray$UnsafeByteArray',
    'com.google.common.hash.LittleEndianByteArray$UnsafeByteArray$1',
    'com.google.common.hash.LittleEndianByteArray$UnsafeByteArray$2',
    'com.google.common.hash.Striped64',
    'com.google.common.hash.Striped64$1',
    'com.google.common.hash.Striped64$Cell',
    'com.google.common.util.concurrent.AbstractFuture$UnsafeAtomicHelper',
    'com.google.common.util.concurrent.AbstractFuture$UnsafeAtomicHelper$1',

    'org.apache.hadoop.thirdparty.com.google.common.cache.Striped64',
    'org.apache.hadoop.thirdparty.com.google.common.cache.Striped64$1',
    'org.apache.hadoop.thirdparty.com.google.common.cache.Striped64$Cell',
    'org.apache.hadoop.thirdparty.com.google.common.hash.LittleEndianByteArray$UnsafeByteArray',
    'org.apache.hadoop.thirdparty.com.google.common.hash.LittleEndianByteArray$UnsafeByteArray$1',
    'org.apache.hadoop.thirdparty.com.google.common.hash.LittleEndianByteArray$UnsafeByteArray$2',
    'org.apache.hadoop.thirdparty.com.google.common.hash.Striped64',
    'org.apache.hadoop.thirdparty.com.google.common.hash.Striped64$1',
    'org.apache.hadoop.thirdparty.com.google.common.hash.Striped64$Cell',
    'org.apache.hadoop.thirdparty.com.google.common.primitives.UnsignedBytes$LexicographicalComparatorHolder$UnsafeComparator',
    'org.apache.hadoop.thirdparty.com.google.common.primitives.UnsignedBytes$LexicographicalComparatorHolder$UnsafeComparator$1',
    'org.apache.hadoop.thirdparty.com.google.common.util.concurrent.AbstractFuture$UnsafeAtomicHelper',
    'org.apache.hadoop.thirdparty.com.google.common.util.concurrent.AbstractFuture$UnsafeAtomicHelper$1',
    'org.apache.hadoop.thirdparty.protobuf.UnsafeUtil',
    'org.apache.hadoop.thirdparty.protobuf.UnsafeUtil$1',
    'org.apache.hadoop.thirdparty.protobuf.UnsafeUtil$JvmMemoryAccessor',
    'org.apache.hadoop.thirdparty.protobuf.UnsafeUtil$MemoryAccessor',
    'org.apache.hadoop.thirdparty.protobuf.MessageSchema',
    'org.apache.hadoop.thirdparty.protobuf.UnsafeUtil$Android32MemoryAccessor',
    'org.apache.hadoop.thirdparty.protobuf.UnsafeUtil$Android64MemoryAccessor',
    'org.apache.hadoop.shaded.com.google.common.cache.Striped64',
    'org.apache.hadoop.shaded.com.google.common.cache.Striped64$1',
    'org.apache.hadoop.shaded.com.google.common.cache.Striped64$Cell',
    'org.apache.hadoop.shaded.com.google.common.hash.LittleEndianByteArray$UnsafeByteArray',
    'org.apache.hadoop.shaded.com.google.common.hash.LittleEndianByteArray$UnsafeByteArray$1',
    'org.apache.hadoop.shaded.com.google.common.hash.LittleEndianByteArray$UnsafeByteArray$2',
    'org.apache.hadoop.shaded.com.google.common.hash.LittleEndianByteArray$UnsafeByteArray$3',
    'org.apache.hadoop.shaded.com.google.common.hash.Striped64',
    'org.apache.hadoop.shaded.com.google.common.hash.Striped64$1',
    'org.apache.hadoop.shaded.com.google.common.hash.Striped64$Cell',
    'org.apache.hadoop.shaded.com.google.common.primitives.UnsignedBytes$LexicographicalComparatorHolder$UnsafeComparator',
    'org.apache.hadoop.shaded.com.google.common.primitives.UnsignedBytes$LexicographicalComparatorHolder$UnsafeComparator$1',
    'org.apache.hadoop.shaded.com.google.common.util.concurrent.AbstractFuture$UnsafeAtomicHelper',
    'org.apache.hadoop.shaded.com.google.common.util.concurrent.AbstractFuture$UnsafeAtomicHelper$1',
    'org.apache.hadoop.shaded.org.apache.avro.reflect.FieldAccessUnsafe',
    'org.apache.hadoop.shaded.org.apache.avro.reflect.FieldAccessUnsafe$UnsafeBooleanField',
    'org.apache.hadoop.shaded.org.apache.avro.reflect.FieldAccessUnsafe$UnsafeByteField',
    'org.apache.hadoop.shaded.org.apache.avro.reflect.FieldAccessUnsafe$UnsafeCachedField',
    'org.apache.hadoop.shaded.org.apache.avro.reflect.FieldAccessUnsafe$UnsafeCharField',
    'org.apache.hadoop.shaded.org.apache.avro.reflect.FieldAccessUnsafe$UnsafeCustomEncodedField',
    'org.apache.hadoop.shaded.org.apache.avro.reflect.FieldAccessUnsafe$UnsafeDoubleField',
    'org.apache.hadoop.shaded.org.apache.avro.reflect.FieldAccessUnsafe$UnsafeFloatField',
    'org.apache.hadoop.shaded.org.apache.avro.reflect.FieldAccessUnsafe$UnsafeIntField',
    'org.apache.hadoop.shaded.org.apache.avro.reflect.FieldAccessUnsafe$UnsafeLongField',
    'org.apache.hadoop.shaded.org.apache.avro.reflect.FieldAccessUnsafe$UnsafeObjectField',
    'org.apache.hadoop.shaded.org.apache.avro.reflect.FieldAccessUnsafe$UnsafeShortField',
    'org.apache.hadoop.shaded.org.apache.curator.shaded.com.google.common.cache.Striped64',
    'org.apache.hadoop.shaded.org.apache.curator.shaded.com.google.common.cache.Striped64$1',
    'org.apache.hadoop.shaded.org.apache.curator.shaded.com.google.common.cache.Striped64$Cell',
    'org.apache.hadoop.shaded.org.apache.curator.shaded.com.google.common.hash.LittleEndianByteArray$UnsafeByteArray',
    'org.apache.hadoop.shaded.org.apache.curator.shaded.com.google.common.hash.LittleEndianByteArray$UnsafeByteArray$1',
    'org.apache.hadoop.shaded.org.apache.curator.shaded.com.google.common.hash.LittleEndianByteArray$UnsafeByteArray$2',
    'org.apache.hadoop.shaded.org.apache.curator.shaded.com.google.common.hash.LittleEndianByteArray$UnsafeByteArray$3',
    'org.apache.hadoop.shaded.org.apache.curator.shaded.com.google.common.hash.Striped64',
    'org.apache.hadoop.shaded.org.apache.curator.shaded.com.google.common.hash.Striped64$1',
    'org.apache.hadoop.shaded.org.apache.curator.shaded.com.google.common.hash.Striped64$Cell',
    'org.apache.hadoop.shaded.org.apache.curator.shaded.com.google.common.primitives.UnsignedBytes$LexicographicalComparatorHolder$UnsafeComparator',
    'org.apache.hadoop.shaded.org.apache.curator.shaded.com.google.common.primitives.UnsignedBytes$LexicographicalComparatorHolder$UnsafeComparator$1',
    'org.apache.hadoop.shaded.org.apache.curator.shaded.com.google.common.util.concurrent.AbstractFuture$UnsafeAtomicHelper',
    'org.apache.hadoop.shaded.org.apache.curator.shaded.com.google.common.util.concurrent.AbstractFuture$UnsafeAtomicHelper$1',
    'org.apache.hadoop.shaded.org.xbill.DNS.spi.DNSJavaNameServiceDescriptor',
  )
}

tasks.withType(JavaForkOptions) {
  systemProperty "java.util.concurrent.ForkJoinPool.common.threadFactory", "org.opensearch.secure_sm.SecuredForkJoinWorkerThreadFactory"
}
